{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from mlp import MLP\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch import nn\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note on Scaling below: \n",
    "        - The test set for SVM has been scaled based on the original training set.\n",
    "        - For MLP a validation set was split from the training set and so the test and validation sets were scaled on that               smaller training set to prevent data leakage. \n",
    "        - This is why there are two different scalings for SVM and MLP below, the test set is the same in both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data and Scale Test Set - SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('Data/X_test.csv')\n",
    "y_test = pd.read_csv('Data/y_test.csv', header = None)\n",
    "X_train = pd.read_csv('Data/X_train.csv')\n",
    "y_train = pd.read_csv('Data/y_train.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train Length: 464,809\n",
      "X_test Length:  116,203\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "y_train Length: 464,809\n",
      "y_test Length:  116,203\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train Length: {:,}\".format(len(X_train)))\n",
    "print(\"X_test Length:  {:,}\".format(len(X_test)))\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "print(\"y_train Length: {:,}\".format(len(y_train)))\n",
    "print(\"y_test Length:  {:,}\".format(len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definining Scaler:\n",
    "scaler = MinMaxScaler()\n",
    "# Fitting scaler on X_train:\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "# Applying to test set:\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to Calculate Overall Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to calculate overall accuracy:\n",
    "def calc_acc_svm(y_pred, y_test):\n",
    "    \n",
    "    # Calculates correct predictions by comparing to actual values\n",
    "    correct_pred = (y_pred == y_test)\n",
    "    accuracy = correct_pred.sum() / len(correct_pred)\n",
    "    \n",
    "    accuracy = accuracy * 100\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to calculate accuracy of predictions intra-batch:\n",
    "\n",
    "def calc_acc_mlp(y_pred, y_test):\n",
    "    # Applies a softmax followed by a log to the y_pred tensor:\n",
    "    y_pred_torch = torch.log_softmax(y_pred, dim = 1)\n",
    "    \n",
    "    # Returns the predicted values for each batch datapoint\n",
    "    _, y_pred_values = torch.max(y_pred_torch, dim = 1)    \n",
    "    \n",
    "    # Calculates correct predictions by comparing to actual values\n",
    "    correct_pred = (y_pred_values == y_test).float()\n",
    "    accuracy = correct_pred.sum() / len(correct_pred)\n",
    "    \n",
    "    accuracy = accuracy * 100\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the best SVM Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model:\n",
    "svm_best_model = pickle.load(open('Models/svm_basic_model.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2084.9346647262573\n"
     ]
    }
   ],
   "source": [
    "# Used to predict the test set, time included as an indication of how long it will take (seconds):\n",
    "start = time.time()\n",
    "y_pred_SVM = svm_best_model.predict(X_test)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86.51755978761305"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overall Accuracy of the Model:\n",
    "calc_acc_svm(y_pred_SVM, y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.88      0.82      0.85     42368\n",
      "           2       0.86      0.91      0.88     56661\n",
      "           3       0.87      0.89      0.88      7151\n",
      "           4       0.88      0.80      0.84       549\n",
      "           5       0.84      0.54      0.66      1899\n",
      "           6       0.80      0.73      0.76      3473\n",
      "           7       0.93      0.90      0.91      4102\n",
      "\n",
      "    accuracy                           0.87    116203\n",
      "   macro avg       0.87      0.80      0.83    116203\n",
      "weighted avg       0.87      0.87      0.86    116203\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing key metrics:\n",
    "print(classification_report(y_test, y_pred_SVM, labels=[1, 2, 3, 4, 5, 6, 7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the best MLP Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Scaling Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mlp = pd.read_csv('Data/X_train_mlp.csv')\n",
    "X_val_mlp = pd.read_csv('Data/X_val_mlp.csv')\n",
    "X_test_mlp = pd.read_csv('Data/X_test.csv')\n",
    "y_train_mlp = pd.read_csv('Data/y_train_mlp.csv', header = None)\n",
    "y_val_mlp = pd.read_csv('Data/y_val_mlp.csv', header = None)\n",
    "y_test_mlp = pd.read_csv('Data/y_test.csv', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train Length: 371,847\n",
      "X_val Length:   92,962\n",
      "X_test Length:  116,203\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "y_train Length: 371,847\n",
      "y_val Length:   92,962\n",
      "y_test Length:  116,203\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train Length: {:,}\".format(len(X_train_mlp)))\n",
    "print(\"X_val Length:   {:,}\".format(len(X_val_mlp)))\n",
    "print(\"X_test Length:  {:,}\".format(len(X_test_mlp)))\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "print(\"y_train Length: {:,}\".format(len(y_train_mlp)))\n",
    "print(\"y_val Length:   {:,}\".format(len(y_val_mlp)))\n",
    "print(\"y_test Length:  {:,}\".format(len(y_test_mlp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definining Scaler:\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fitting scaler on X_train:\n",
    "X_train_mlp = scaler.fit_transform(X_train_mlp)\n",
    "\n",
    "# Applying this scaling to the test set:\n",
    "X_test_mlp = scaler.transform(X_test_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to tensors:\n",
    "X_test_tensor = torch.from_numpy(X_test_mlp).float()\n",
    "y_test_tensor = torch.from_numpy(y_test_mlp.values).long() - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataloader Object to load the test set in batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataset class based on torch.utils.data.Dataset that can be loaded using train_loader\n",
    "# source: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "\n",
    "class CreatePytorchDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "    \n",
    "    # Altering __len__ and __getitem__ methods from Dataset, so len returns size of dataset and get item \n",
    "    # allows dataset indexing by converting any tensor to a list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_data[idx], self.y_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Pytorch Dataset using the imported function above:\n",
    "test_dataset = CreatePytorchDataset(X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataloader object:\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Testing the Best Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the saved best model:\n",
    "mlp_best_model = pickle.load(open('Models/mlp_sampled_model.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing Cross Entropy Loss which combines logsoftmax and NLLLoss for use in multi-class classification:\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.180 | Test Acc: 93.506\n",
      "Time taken (seconds):  1.9268395900726318\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Setting the cumulative loss and accuracy to zero, to be updated within for loop below.\n",
    "cum_test_loss = 0\n",
    "cum_test_acc = 0\n",
    "\n",
    "# Creating lists that will hold all predicted and actual target values tested\n",
    "all_preds_list = []\n",
    "all_actual_list = []\n",
    "\n",
    "    \n",
    "# TESTING #\n",
    "mlp_best_model.eval()\n",
    "\n",
    "for data, target in test_loader:\n",
    "    # Predictions on test set batches\n",
    "    y_pred_batch = mlp_best_model(data)\n",
    "\n",
    "    # Cross-Entropy Loss and Accuracy:\n",
    "    test_batch_loss = criterion(y_pred_batch, target.flatten())\n",
    "    test_batch_acc = calc_acc_mlp(y_pred_batch, target.flatten())\n",
    "\n",
    "    # Updating cumulative running totals:\n",
    "    cum_test_loss += test_batch_loss.item()\n",
    "    cum_test_acc += test_batch_acc.item()\n",
    "\n",
    "    # Adding predictions/actual target to lists for confusion matrix:\n",
    "    # Applies a softmax followed by a log to the y_pred tensor:\n",
    "    y_pred_softmax = torch.log_softmax(y_pred_batch, dim = 1)\n",
    "    # Returns the predicted values for each batch datapoint (the maximums within each logsoftmax'd tensor)\n",
    "    _, y_pred_values = torch.max(y_pred_softmax, dim = 1) \n",
    "\n",
    "    # Converting predictions and actual target and storing in lists above:\n",
    "    all_preds_list += (list(y_pred_values.numpy()))\n",
    "    all_actual_list += (list(target.flatten().numpy()))\n",
    "\n",
    "# PRINTING STATISTICS #\n",
    "avg_test_loss = cum_test_loss / len(test_loader)\n",
    "avg_test_acc = cum_test_acc / len(test_loader)\n",
    "\n",
    "\n",
    "# Display:\n",
    "print('Test Loss: {:.3f} | Test Acc: {:.3f}'.format(avg_test_loss, avg_test_acc))\n",
    "        \n",
    "print(\"Time taken (seconds): \", time.time() - start)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93     42368\n",
      "           1       0.96      0.93      0.94     56661\n",
      "           2       0.94      0.96      0.95      7151\n",
      "           3       0.85      0.91      0.88       549\n",
      "           4       0.68      0.96      0.80      1899\n",
      "           5       0.89      0.95      0.92      3473\n",
      "           6       0.90      0.98      0.94      4102\n",
      "\n",
      "    accuracy                           0.94    116203\n",
      "   macro avg       0.88      0.95      0.91    116203\n",
      "weighted avg       0.94      0.94      0.94    116203\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(all_actual_list, all_preds_list, labels=[0, 1, 2, 3, 4, 5, 6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
