{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, make_scorer, accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Set:\n",
    "X_train = pd.read_csv('Data/X_train.csv')\n",
    "y_train = pd.read_csv('Data/y_train.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train Length: 464,809\n",
      "y_train Length: 464,809\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train Length: {:,}\".format(len(X_train)))\n",
    "print(\"y_train Length: {:,}\".format(len(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definining Scaler:\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fitting scaler on X_train:\n",
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting y_train into an array:\n",
    "y_train = np.array(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using train-test split to split out a random, stratified 10% portion of the training data to grid search:\n",
    "X_gs, X_leave, y_gs, y_leave = train_test_split(X_train, y_train, \n",
    "                                                test_size = 0.9,  # Inverse of the size of the data to search\n",
    "                                                stratify = y_train)  # Keeping same proportion of target classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A 10% stratified subset will be used to search for hyperparameters, see the length of X_train/y_test below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Cross-Val Length: 46,480\n",
      "y Cross-Val Length: 46,480\n"
     ]
    }
   ],
   "source": [
    "print(\"X Cross-Val Length: {:,}\".format(len(X_gs)))\n",
    "print(\"y Cross-Val Length: {:,}\".format(len(y_gs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pytorch Dataloader Datasets:\n",
    "- See MLP Notebook for full explanation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataset class based on torch.utils.data.Dataset that can be loaded using train_loader\n",
    "# source: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "\n",
    "class CreatePytorchDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_data[idx], self.y_data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Calculate Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_acc(y_pred, y_test):\n",
    "    y_pred_torch = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_values = torch.max(y_pred_torch, dim = 1)    \n",
    "    correct_pred = (y_pred_values == y_test).float()\n",
    "    accuracy = correct_pred.sum() / len(correct_pred)\n",
    "    accuracy = accuracy * 100\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining The Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_features, n_classes, n_neurons, activ, drop):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # Define the layers: \n",
    "        # Defining the linear tranformations:\n",
    "        self.fc1 = nn.Linear(n_features, n_neurons)\n",
    "        self.fc2 = nn.Linear(n_neurons, int(n_neurons/2))\n",
    "        self.fc3 = nn.Linear(int(n_neurons/2), int(n_neurons/4))\n",
    "        \n",
    "        # Output layer\n",
    "        self.output = nn.Linear(int(n_neurons/4), n_classes)\n",
    "       \n",
    "        # Batch normalisation layers \n",
    "        # (https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html?highlight=batchnorm#torch.nn.BatchNorm1d)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(n_neurons)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(int(n_neurons/2))\n",
    "        self.batchnorm3 = nn.BatchNorm1d(int(n_neurons/4))\n",
    "    \n",
    "        # Defining dropout rate:\n",
    "        self.dropout = nn.Dropout(drop) \n",
    "        \n",
    "        # Defining activation function\n",
    "        self.activ = activ\n",
    "        \n",
    "        \n",
    "    # Define forward function\n",
    "    def forward(self, x):\n",
    "        hidden = self.fc1(x)\n",
    "        hidden = self.batchnorm1(hidden)\n",
    "        hidden = self.activ(hidden)\n",
    "        \n",
    "        hidden = self.fc2(hidden)\n",
    "        hidden = self.batchnorm2(hidden)\n",
    "        hidden = self.activ(hidden)  \n",
    "        hidden = self.dropout(hidden)\n",
    "        \n",
    "        hidden = self.fc3(hidden)\n",
    "        hidden = self.batchnorm3(hidden)\n",
    "        hidden = self.activ(hidden)\n",
    "        hidden = self.dropout(hidden)\n",
    "        \n",
    "        output = self.output(hidden)\n",
    "        \n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Within Cross Validation Loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_loop(model, n_epochs, train_loader, val_loader, criterion, optimizer):\n",
    "    \n",
    "    # To track lowest validation loss through epochs, set initially to infinity:\n",
    "    min_val_loss = np.Inf\n",
    "\n",
    "    # Lists to hold accuracies and losses through epochs:\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "    val_loss_list = []\n",
    "    val_acc_list = []\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        \n",
    "        # Setting initial losses/accuracy to zero, to be updated within batch loops\n",
    "        train_loss_epoch = 0\n",
    "        train_acc_epoch = 0\n",
    "        val_loss_epoch = 0\n",
    "        val_acc_epoch = 0\n",
    "\n",
    "        # TRAINING #\n",
    "        model.train()\n",
    "\n",
    "        for data, target in train_loader:\n",
    "            # Set gradients of model parameters to zero:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Making predictions based on the training data batch\n",
    "            y_pred_batch = model(data)\n",
    "\n",
    "            # Cross-Entropy Loss:\n",
    "            train_batch_loss = criterion(y_pred_batch, target.flatten())\n",
    "\n",
    "            # Classifications correct within batch, to use to calculate overall accuracy:\n",
    "            train_batch_acc = calc_acc(y_pred_batch, target.flatten())\n",
    "\n",
    "            # Calculate the sum of gradients using backward:\n",
    "            train_batch_loss.backward()\n",
    "\n",
    "            # Update parameters by performing a single optimization:\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the running loss/accuracy totals:\n",
    "            train_loss_epoch += train_batch_loss.item()\n",
    "            train_acc_epoch += train_batch_acc.item()\n",
    "\n",
    "        # Disabling gradient calculation to speed up process, as backward will not be called during validation:\n",
    "        with torch.no_grad():\n",
    "            # VALIDATION #\n",
    "            model.eval()\n",
    "\n",
    "            for data, target in val_loader:\n",
    "                # Use the model to predict target in val set batch:\n",
    "                y_pred_val_batch =  model(data)\n",
    "\n",
    "                # Calculate loss and accuracy on val set:\n",
    "                val_batch_loss = criterion(y_pred_val_batch, target.flatten())\n",
    "                val_batch_acc = calc_acc(y_pred_val_batch, target.flatten())\n",
    "\n",
    "                # Update the running loss/accuracy totals:\n",
    "                val_loss_epoch += train_batch_loss.item()\n",
    "                val_acc_epoch += train_batch_acc.item()\n",
    "\n",
    "        # PRINTING STATISTICS #\n",
    "\n",
    "        # Calculate metrics across epoch:\n",
    "        avg_train_loss = train_loss_epoch/ len(train_loader)\n",
    "        avg_train_acc = train_acc_epoch/ len(train_loader)\n",
    "\n",
    "        avg_val_loss = val_loss_epoch/ len(val_loader)\n",
    "        avg_val_acc = val_acc_epoch/ len(val_loader)\n",
    "\n",
    "        # Display:\n",
    "        #print('Epoch {}: | Train Loss: {:.3f} | Train Acc: {:.3f} | Val Loss: {:.3f}| Val Acc:{:.3f}'.format(\n",
    "            #epoch, avg_train_loss, avg_train_acc, avg_val_loss, avg_val_acc))\n",
    "\n",
    "        # Appending lists outside epoch loop to store results:\n",
    "        train_loss_list.append(avg_train_loss)\n",
    "        train_acc_list.append(avg_train_acc)\n",
    "        val_loss_list.append(avg_val_loss)\n",
    "        val_acc_list.append(avg_val_acc)\n",
    "\n",
    "        if avg_val_loss <= min_val_loss:\n",
    "            # Alter min loss to current avg_val_loss:\n",
    "            min_val_loss = avg_val_loss\n",
    "        \n",
    "        # EARLY STOPPING CRITERIA #\n",
    "        # If validation loss has increased for 5 consequetive epochs, end loop:\n",
    "        # Only run if > 10 epochs gone\n",
    "        if epoch > 10:\n",
    "            # Checks if the last 5 val_losses are all larger than the preceeding one:\n",
    "            if ((val_loss_list[-1]>val_loss_list[-2]) and (val_loss_list[-2]>val_loss_list[-3]) and \n",
    "                (val_loss_list[-3]>val_loss_list[-4]) and (val_loss_list[-4]>val_loss_list[-5]) and \n",
    "                (val_loss_list[-5]>val_loss_list[-6])):\n",
    "                break \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    return min_val_loss, epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation Loops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting static variables:\n",
    "\n",
    "N_CLASSES = 7\n",
    "N_INPUT_FEAT = 54\n",
    "CRITERION = nn.CrossEntropyLoss()\n",
    "EVAL_METRIC = make_scorer(accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL LOOPS =  360\n",
      "Starting Loop 0\n",
      "Starting Loop 1\n",
      "Starting Loop 2\n",
      "Starting Loop 3\n",
      "Starting Loop 4\n",
      "Starting Loop 5\n",
      "Starting Loop 6\n",
      "Starting Loop 7\n",
      "Starting Loop 8\n",
      "Starting Loop 9\n",
      "Starting Loop 10\n",
      "Starting Loop 11\n",
      "Starting Loop 12\n",
      "Starting Loop 13\n",
      "Starting Loop 14\n",
      "Starting Loop 15\n",
      "Starting Loop 16\n",
      "Starting Loop 17\n",
      "Starting Loop 18\n",
      "Starting Loop 19\n",
      "Starting Loop 20\n",
      "Starting Loop 21\n",
      "Starting Loop 22\n",
      "Starting Loop 23\n",
      "Starting Loop 24\n",
      "Starting Loop 25\n",
      "Starting Loop 26\n",
      "Starting Loop 27\n",
      "Starting Loop 28\n",
      "Starting Loop 29\n",
      "Starting Loop 30\n",
      "Starting Loop 31\n",
      "Starting Loop 32\n",
      "Starting Loop 33\n",
      "Starting Loop 34\n",
      "Starting Loop 35\n",
      "Starting Loop 36\n",
      "Starting Loop 37\n",
      "Starting Loop 38\n",
      "Starting Loop 39\n",
      "Starting Loop 40\n",
      "Starting Loop 41\n",
      "Starting Loop 42\n",
      "Starting Loop 43\n",
      "Starting Loop 44\n",
      "Starting Loop 45\n",
      "Starting Loop 46\n",
      "Starting Loop 47\n",
      "Starting Loop 48\n",
      "Starting Loop 49\n",
      "Starting Loop 50\n",
      "Starting Loop 51\n",
      "Starting Loop 52\n",
      "Starting Loop 53\n",
      "Starting Loop 54\n",
      "Starting Loop 55\n",
      "Starting Loop 56\n",
      "Starting Loop 57\n",
      "Starting Loop 58\n",
      "Starting Loop 59\n",
      "Starting Loop 60\n",
      "Starting Loop 61\n",
      "Starting Loop 62\n",
      "Starting Loop 63\n",
      "Starting Loop 64\n",
      "Starting Loop 65\n",
      "Starting Loop 66\n",
      "Starting Loop 67\n",
      "Starting Loop 68\n",
      "Starting Loop 69\n",
      "Starting Loop 70\n",
      "Starting Loop 71\n",
      "Starting Loop 72\n",
      "Starting Loop 73\n",
      "Starting Loop 74\n",
      "Starting Loop 75\n",
      "Starting Loop 76\n",
      "Starting Loop 77\n",
      "Starting Loop 78\n",
      "Starting Loop 79\n",
      "Starting Loop 80\n",
      "Starting Loop 81\n",
      "Starting Loop 82\n",
      "Starting Loop 83\n",
      "Starting Loop 84\n",
      "Starting Loop 85\n",
      "Starting Loop 86\n",
      "Starting Loop 87\n",
      "Starting Loop 88\n",
      "Starting Loop 89\n",
      "Starting Loop 90\n",
      "Starting Loop 91\n",
      "Starting Loop 92\n",
      "Starting Loop 93\n",
      "Starting Loop 94\n",
      "Starting Loop 95\n",
      "Starting Loop 96\n",
      "Starting Loop 97\n",
      "Starting Loop 98\n",
      "Starting Loop 99\n",
      "Starting Loop 100\n",
      "Starting Loop 101\n",
      "Starting Loop 102\n",
      "Starting Loop 103\n",
      "Starting Loop 104\n",
      "Starting Loop 105\n",
      "Starting Loop 106\n",
      "Starting Loop 107\n",
      "Starting Loop 108\n",
      "Starting Loop 109\n",
      "Starting Loop 110\n",
      "Starting Loop 111\n",
      "Starting Loop 112\n",
      "Starting Loop 113\n",
      "Starting Loop 114\n",
      "Starting Loop 115\n",
      "Starting Loop 116\n",
      "Starting Loop 117\n",
      "Starting Loop 118\n",
      "Starting Loop 119\n",
      "Starting Loop 120\n",
      "Starting Loop 121\n",
      "Starting Loop 122\n",
      "Starting Loop 123\n",
      "Starting Loop 124\n",
      "Starting Loop 125\n",
      "Starting Loop 126\n",
      "Starting Loop 127\n",
      "Starting Loop 128\n",
      "Starting Loop 129\n",
      "Starting Loop 130\n",
      "Starting Loop 131\n",
      "Starting Loop 132\n",
      "Starting Loop 133\n",
      "Starting Loop 134\n",
      "Starting Loop 135\n",
      "Starting Loop 136\n",
      "Starting Loop 137\n",
      "Starting Loop 138\n",
      "Starting Loop 139\n",
      "Starting Loop 140\n",
      "Starting Loop 141\n",
      "Starting Loop 142\n",
      "Starting Loop 143\n",
      "Starting Loop 144\n",
      "Starting Loop 145\n",
      "Starting Loop 146\n",
      "Starting Loop 147\n",
      "Starting Loop 148\n",
      "Starting Loop 149\n",
      "Starting Loop 150\n",
      "Starting Loop 151\n",
      "Starting Loop 152\n",
      "Starting Loop 153\n",
      "Starting Loop 154\n",
      "Starting Loop 155\n",
      "Starting Loop 156\n",
      "Starting Loop 157\n",
      "Starting Loop 158\n",
      "Starting Loop 159\n",
      "Starting Loop 160\n",
      "Starting Loop 161\n",
      "Starting Loop 162\n",
      "Starting Loop 163\n",
      "Starting Loop 164\n",
      "Starting Loop 165\n",
      "Starting Loop 166\n",
      "Starting Loop 167\n",
      "Starting Loop 168\n",
      "Starting Loop 169\n",
      "Starting Loop 170\n",
      "Starting Loop 171\n",
      "Starting Loop 172\n",
      "Starting Loop 173\n",
      "Starting Loop 174\n",
      "Starting Loop 175\n",
      "Starting Loop 176\n",
      "Starting Loop 177\n",
      "Starting Loop 178\n",
      "Starting Loop 179\n",
      "Starting Loop 180\n",
      "Starting Loop 181\n",
      "Starting Loop 182\n",
      "Starting Loop 183\n",
      "Starting Loop 184\n",
      "Starting Loop 185\n",
      "Starting Loop 186\n",
      "Starting Loop 187\n",
      "Starting Loop 188\n",
      "Starting Loop 189\n",
      "Starting Loop 190\n",
      "Starting Loop 191\n",
      "Starting Loop 192\n",
      "Starting Loop 193\n",
      "Starting Loop 194\n",
      "Starting Loop 195\n",
      "Starting Loop 196\n",
      "Starting Loop 197\n",
      "Starting Loop 198\n",
      "Starting Loop 199\n",
      "Starting Loop 200\n",
      "Starting Loop 201\n",
      "Starting Loop 202\n",
      "Starting Loop 203\n",
      "Starting Loop 204\n",
      "Starting Loop 205\n",
      "Starting Loop 206\n",
      "Starting Loop 207\n",
      "Starting Loop 208\n",
      "Starting Loop 209\n",
      "Starting Loop 210\n",
      "Starting Loop 211\n",
      "Starting Loop 212\n",
      "Starting Loop 213\n",
      "Starting Loop 214\n",
      "Starting Loop 215\n",
      "Starting Loop 216\n",
      "Starting Loop 217\n",
      "Starting Loop 218\n",
      "Starting Loop 219\n",
      "Starting Loop 220\n",
      "Starting Loop 221\n",
      "Starting Loop 222\n",
      "Starting Loop 223\n",
      "Starting Loop 224\n",
      "Starting Loop 225\n",
      "Starting Loop 226\n",
      "Starting Loop 227\n",
      "Starting Loop 228\n",
      "Starting Loop 229\n",
      "Starting Loop 230\n",
      "Starting Loop 231\n",
      "Starting Loop 232\n",
      "Starting Loop 233\n",
      "Starting Loop 234\n",
      "Starting Loop 235\n",
      "Starting Loop 236\n",
      "Starting Loop 237\n",
      "Starting Loop 238\n",
      "Starting Loop 239\n",
      "Starting Loop 240\n",
      "Starting Loop 241\n",
      "Starting Loop 242\n",
      "Starting Loop 243\n",
      "Starting Loop 244\n",
      "Starting Loop 245\n",
      "Starting Loop 246\n",
      "Starting Loop 247\n",
      "Starting Loop 248\n",
      "Starting Loop 249\n",
      "Starting Loop 250\n",
      "Starting Loop 251\n",
      "Starting Loop 252\n",
      "Starting Loop 253\n",
      "Starting Loop 254\n",
      "Starting Loop 255\n",
      "Starting Loop 256\n",
      "Starting Loop 257\n",
      "Starting Loop 258\n",
      "Starting Loop 259\n",
      "Starting Loop 260\n",
      "Starting Loop 261\n",
      "Starting Loop 262\n",
      "Starting Loop 263\n",
      "Starting Loop 264\n",
      "Starting Loop 265\n",
      "Starting Loop 266\n",
      "Starting Loop 267\n",
      "Starting Loop 268\n",
      "Starting Loop 269\n",
      "Starting Loop 270\n",
      "Starting Loop 271\n",
      "Starting Loop 272\n",
      "Starting Loop 273\n",
      "Starting Loop 274\n",
      "Starting Loop 275\n",
      "Starting Loop 276\n",
      "Starting Loop 277\n",
      "Starting Loop 278\n",
      "Starting Loop 279\n",
      "Starting Loop 280\n",
      "Starting Loop 281\n",
      "Starting Loop 282\n",
      "Starting Loop 283\n",
      "Starting Loop 284\n",
      "Starting Loop 285\n",
      "Starting Loop 286\n",
      "Starting Loop 287\n",
      "Starting Loop 288\n",
      "Starting Loop 289\n",
      "Starting Loop 290\n",
      "Starting Loop 291\n",
      "Starting Loop 292\n",
      "Starting Loop 293\n",
      "Starting Loop 294\n",
      "Starting Loop 295\n",
      "Starting Loop 296\n",
      "Starting Loop 297\n",
      "Starting Loop 298\n",
      "Starting Loop 299\n",
      "Starting Loop 300\n",
      "Starting Loop 301\n",
      "Starting Loop 302\n",
      "Starting Loop 303\n",
      "Starting Loop 304\n",
      "Starting Loop 305\n",
      "Starting Loop 306\n",
      "Starting Loop 307\n",
      "Starting Loop 308\n",
      "Starting Loop 309\n",
      "Starting Loop 310\n",
      "Starting Loop 311\n",
      "Starting Loop 312\n",
      "Starting Loop 313\n",
      "Starting Loop 314\n",
      "Starting Loop 315\n",
      "Starting Loop 316\n",
      "Starting Loop 317\n",
      "Starting Loop 318\n",
      "Starting Loop 319\n",
      "Starting Loop 320\n",
      "Starting Loop 321\n",
      "Starting Loop 322\n",
      "Starting Loop 323\n",
      "Starting Loop 324\n",
      "Starting Loop 325\n",
      "Starting Loop 326\n",
      "Starting Loop 327\n",
      "Starting Loop 328\n",
      "Starting Loop 329\n",
      "Starting Loop 330\n",
      "Starting Loop 331\n",
      "Starting Loop 332\n",
      "Starting Loop 333\n",
      "Starting Loop 334\n",
      "Starting Loop 335\n",
      "Starting Loop 336\n",
      "Starting Loop 337\n",
      "Starting Loop 338\n",
      "Starting Loop 339\n",
      "Starting Loop 340\n",
      "Starting Loop 341\n",
      "Starting Loop 342\n",
      "Starting Loop 343\n",
      "Starting Loop 344\n",
      "Starting Loop 345\n",
      "Starting Loop 346\n",
      "Starting Loop 347\n",
      "Starting Loop 348\n",
      "Starting Loop 349\n",
      "Starting Loop 350\n",
      "Starting Loop 351\n",
      "Starting Loop 352\n",
      "Starting Loop 353\n",
      "Starting Loop 354\n",
      "Starting Loop 355\n",
      "Starting Loop 356\n",
      "Starting Loop 357\n",
      "Starting Loop 358\n",
      "Starting Loop 359\n"
     ]
    }
   ],
   "source": [
    "# Setting parameter values to search through:\n",
    "drop_out = [0, 0.1, 0.2, 0.3]            \n",
    "max_neurons = [64, 128, 256, 512]           \n",
    "learn_rate = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]     \n",
    "moment = [0, 1e-2, 1e-3, 0.999]           \n",
    "\n",
    "# Creating an array to store results from the searches:\n",
    "test_results = np.zeros(((len(drop_out)*len(max_neurons)*len(learn_rate)*len(moment)), 11))\n",
    "\n",
    "max_epochs = 100\n",
    "n_folds = 4\n",
    "kf = KFold(n_splits = n_folds,shuffle = False)\n",
    "CRITERION = nn.CrossEntropyLoss()\n",
    "EVAL_METRIC = make_scorer(accuracy_score)\n",
    "loop_counter = 0\n",
    "print(\"TOTAL LOOPS = \", (len(drop_out)*len(max_neurons)*len(learn_rate)*len(moment)*n_folds))\n",
    "\n",
    "# Looping through parameters and performing cross-validation within those loops:\n",
    "for m in range(len(moment)):\n",
    "    \n",
    "    for l in range(len(learn_rate)):\n",
    "\n",
    "        for n in range(len(max_neurons)):\n",
    "\n",
    "            for i in range(len(drop_out)):\n",
    "\n",
    "                ### ALTERING MAX NEURONS AND DROP OUT PARAMETERS ###\n",
    "                mlp_model = MLP(n_features = 54, n_classes = 7, \n",
    "                                n_neurons = max_neurons[n], \n",
    "                                activ = nn.LeakyReLU(), \n",
    "                                drop = drop_out[i])\n",
    "\n",
    "                ### ALTERING LEARNING RATE AND MOMENTUM PARAMETERS ###\n",
    "                # Optimizer changed manually between Adam and SGD and cell re-run, results stored to csv below #\n",
    "                optimizer = torch.optim.Adam(mlp_model.parameters(), lr = learn_rate[l], weight_decay = moment[m])\n",
    "\n",
    "                val_losses = 0\n",
    "                epochs = 0\n",
    "                vll = []\n",
    "                \n",
    "                start = time.process_time()\n",
    "                \n",
    "                for train_index, test_index in kf.split(X_gs):                    \n",
    "\n",
    "                    # Obtaining the indexes to split the data for each Cross Validation Fold:\n",
    "                    X_train_CV, X_val_CV = X_gs[train_index], X_gs[test_index]\n",
    "                    y_train_CV, y_val_CV = y_gs[train_index], y_gs[test_index]\n",
    "\n",
    "                    # Converting these fold splits into tensors:\n",
    "                    X_train_CV_tensor = torch.from_numpy(X_train_CV).float()\n",
    "                    X_val_CV_tensor = torch.from_numpy(X_val_CV).float()\n",
    "                    y_train_CV_tensor = torch.from_numpy(y_train_CV).long() - 1\n",
    "                    y_val_CV_tensor = torch.from_numpy(y_val_CV).long() - 1\n",
    "\n",
    "                    # Creating combined datasets to feed into Data Loader:\n",
    "                    train_CV_dataset = CreatePytorchDataset(X_train_CV_tensor, y_train_CV_tensor)\n",
    "                    val_CV_dataset = CreatePytorchDataset(X_val_CV_tensor, y_val_CV_tensor)\n",
    "\n",
    "                    # Creating Data Loader Objects:\n",
    "                    train_CV_loader = DataLoader(train_CV_dataset, batch_size=64, shuffle = True)\n",
    "                    val_CV_loader = DataLoader(val_CV_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "                    print(\"Starting Loop {:.0f}\".format(loop_counter))\n",
    "                    loop_counter += 1\n",
    "\n",
    "                    ######## EPOCH MODEL TRAINING ########\n",
    "                    min_val_loss_epochs, epochs_taken = cross_val_loop(mlp_model, max_epochs, \n",
    "                                                         train_CV_loader, val_CV_loader, CRITERION, optimizer)\n",
    "\n",
    "                    val_losses += min_val_loss_epochs\n",
    "                    epochs += epochs_taken\n",
    "                    vll.append(min_val_loss_epochs)\n",
    "                    \n",
    "                    #print(\"Time to complete CV Loop: {:.1f} minutes\".format((time.process_time() - start)/60))\n",
    "\n",
    "                av_val_loss = val_losses/ n_folds\n",
    "                av_epochs = epochs / n_folds\n",
    "                time_taken = time.process_time() - start\n",
    "                \n",
    "                # Storing results in array created outside loop:\n",
    "                test_results[i+(n*len(drop_out))+(l*(len(drop_out)*len(max_neurons))+ \n",
    "                                (m*(len(drop_out)*len(max_neurons)*len(learn_rate))))][0] = av_val_loss\n",
    "                test_results[i+(n*len(drop_out))+(l*(len(drop_out)*len(max_neurons))+\n",
    "                                (m*(len(drop_out)*len(max_neurons)*len(learn_rate))))][1] = drop_out[i]\n",
    "                test_results[i+(n*len(drop_out))+(l*(len(drop_out)*len(max_neurons))+\n",
    "                                (m*(len(drop_out)*len(max_neurons)*len(learn_rate))))][2] = max_neurons[n]\n",
    "                test_results[i+(n*len(drop_out))+(l*(len(drop_out)*len(max_neurons))+\n",
    "                                (m*(len(drop_out)*len(max_neurons)*len(learn_rate))))][3] = learn_rate[l]\n",
    "                test_results[i+(n*len(drop_out))+(l*(len(drop_out)*len(max_neurons))+\n",
    "                                (m*(len(drop_out)*len(max_neurons)*len(learn_rate))))][4] = moment[m]\n",
    "                test_results[i+(n*len(drop_out))+(l*(len(drop_out)*len(max_neurons))+\n",
    "                                (m*(len(drop_out)*len(max_neurons)*len(learn_rate))))][5] = time_taken\n",
    "                test_results[i+(n*len(drop_out))+(l*(len(drop_out)*len(max_neurons))+\n",
    "                                (m*(len(drop_out)*len(max_neurons)*len(learn_rate))))][6] = av_epochs\n",
    "                test_results[i+(n*len(drop_out))+(l*(len(drop_out)*len(max_neurons))+\n",
    "                                (m*(len(drop_out)*len(max_neurons)*len(learn_rate))))][7] = vll[0]\n",
    "                test_results[i+(n*len(drop_out))+(l*(len(drop_out)*len(max_neurons))+\n",
    "                                (m*(len(drop_out)*len(max_neurons)*len(learn_rate))))][8] = vll[1]\n",
    "                test_results[i+(n*len(drop_out))+(l*(len(drop_out)*len(max_neurons))+\n",
    "                                (m*(len(drop_out)*len(max_neurons)*len(learn_rate))))][9] = vll[2]\n",
    "                test_results[i+(n*len(drop_out))+(l*(len(drop_out)*len(max_neurons))+\n",
    "                                (m*(len(drop_out)*len(max_neurons)*len(learn_rate))))][10] = vll[3]\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Results to Dataframe/ csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res_df= pd.DataFrame(test_results, columns= ['av_val_loss', 'drop_out', 'max_neurons',\n",
    "                            'learn_rate', 'weight_decay', 'time_taken', 'av_epochs', 'cv_1_val_loss',\n",
    "                                'cv_2_val_loss', 'cv_3_val_loss', 'cv_4_val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>av_val_loss</th>\n",
       "      <th>drop_out</th>\n",
       "      <th>max_neurons</th>\n",
       "      <th>learn_rate</th>\n",
       "      <th>momentum</th>\n",
       "      <th>time_taken</th>\n",
       "      <th>av_epochs</th>\n",
       "      <th>cv_1_val_loss</th>\n",
       "      <th>cv_2_val_loss</th>\n",
       "      <th>cv_3_val_loss</th>\n",
       "      <th>cv_4_val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.200918</td>\n",
       "      <td>0.1</td>\n",
       "      <td>512</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3345.125000</td>\n",
       "      <td>78.75</td>\n",
       "      <td>0.278339</td>\n",
       "      <td>0.164305</td>\n",
       "      <td>0.160212</td>\n",
       "      <td>0.200817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.201888</td>\n",
       "      <td>0.0</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1764.468750</td>\n",
       "      <td>72.50</td>\n",
       "      <td>0.238721</td>\n",
       "      <td>0.234384</td>\n",
       "      <td>0.173371</td>\n",
       "      <td>0.161074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.206505</td>\n",
       "      <td>0.0</td>\n",
       "      <td>512</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2301.171875</td>\n",
       "      <td>55.00</td>\n",
       "      <td>0.286845</td>\n",
       "      <td>0.200367</td>\n",
       "      <td>0.155842</td>\n",
       "      <td>0.182966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.217063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>512</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2873.140625</td>\n",
       "      <td>68.75</td>\n",
       "      <td>0.245914</td>\n",
       "      <td>0.231567</td>\n",
       "      <td>0.215597</td>\n",
       "      <td>0.175172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.235044</td>\n",
       "      <td>0.0</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1963.203125</td>\n",
       "      <td>80.00</td>\n",
       "      <td>0.290983</td>\n",
       "      <td>0.276656</td>\n",
       "      <td>0.207501</td>\n",
       "      <td>0.165034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.247868</td>\n",
       "      <td>0.0</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1207.093750</td>\n",
       "      <td>76.25</td>\n",
       "      <td>0.256995</td>\n",
       "      <td>0.259954</td>\n",
       "      <td>0.293576</td>\n",
       "      <td>0.180946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.258694</td>\n",
       "      <td>0.0</td>\n",
       "      <td>512</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3092.828125</td>\n",
       "      <td>73.75</td>\n",
       "      <td>0.321361</td>\n",
       "      <td>0.300024</td>\n",
       "      <td>0.208216</td>\n",
       "      <td>0.205175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.260151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1438.000000</td>\n",
       "      <td>57.50</td>\n",
       "      <td>0.364366</td>\n",
       "      <td>0.217242</td>\n",
       "      <td>0.198681</td>\n",
       "      <td>0.260313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.264219</td>\n",
       "      <td>0.1</td>\n",
       "      <td>512</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2575.828125</td>\n",
       "      <td>60.00</td>\n",
       "      <td>0.354458</td>\n",
       "      <td>0.235324</td>\n",
       "      <td>0.244575</td>\n",
       "      <td>0.222520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.269248</td>\n",
       "      <td>0.1</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1427.578125</td>\n",
       "      <td>57.50</td>\n",
       "      <td>0.294320</td>\n",
       "      <td>0.317084</td>\n",
       "      <td>0.186814</td>\n",
       "      <td>0.278773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   av_val_loss  drop_out  max_neurons  learn_rate  momentum   time_taken  \\\n",
       "0     0.200918       0.1          512      0.0100     0.000  3345.125000   \n",
       "1     0.201888       0.0          256      0.0010     0.000  1764.468750   \n",
       "2     0.206505       0.0          512      0.0010     0.000  2301.171875   \n",
       "3     0.217063       0.0          512      0.0100     0.000  2873.140625   \n",
       "4     0.235044       0.0          256      0.0100     0.000  1963.203125   \n",
       "5     0.247868       0.0          128      0.0100     0.000  1207.093750   \n",
       "6     0.258694       0.0          512      0.0001     0.000  3092.828125   \n",
       "7     0.260151       0.0          256      0.0010     0.001  1438.000000   \n",
       "8     0.264219       0.1          512      0.0010     0.000  2575.828125   \n",
       "9     0.269248       0.1          256      0.0010     0.000  1427.578125   \n",
       "\n",
       "   av_epochs  cv_1_val_loss  cv_2_val_loss  cv_3_val_loss  cv_4_val_loss  \n",
       "0      78.75       0.278339       0.164305       0.160212       0.200817  \n",
       "1      72.50       0.238721       0.234384       0.173371       0.161074  \n",
       "2      55.00       0.286845       0.200367       0.155842       0.182966  \n",
       "3      68.75       0.245914       0.231567       0.215597       0.175172  \n",
       "4      80.00       0.290983       0.276656       0.207501       0.165034  \n",
       "5      76.25       0.256995       0.259954       0.293576       0.180946  \n",
       "6      73.75       0.321361       0.300024       0.208216       0.205175  \n",
       "7      57.50       0.364366       0.217242       0.198681       0.260313  \n",
       "8      60.00       0.354458       0.235324       0.244575       0.222520  \n",
       "9      57.50       0.294320       0.317084       0.186814       0.278773  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_res_df.sort_values(by='av_val_loss').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_res_df.to_csv('Data/MLP_Activ_Data/MLP_ADAM_GS_RELU.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
